{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfccda7-424c-4679-b320-5c94bfcc001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\daree\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\daree\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\daree\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\daree\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\daree\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\daree\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daree\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daree\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daree\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn nltk xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395c8551-a201-4d42-8239-d2e75336cc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n",
      "\n",
      "Original Dataset Shape: (17880, 16)\n",
      "Class Distribution (0=Real, 1=Fake):\n",
      "is_fake\n",
      "0    17014\n",
      "1      866\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Train/Test Split: 14304 / 3576\n",
      "Train Fake Ratio: 0.0484\n",
      "----------------------------------------\n",
      "Training Logistic Regression Model...\n",
      "Training complete.\n",
      "\n",
      "## üéØ Logistic Regression Model Evaluation (with SMOTE) ##\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3403\n",
      "           1       0.76      0.90      0.82       173\n",
      "\n",
      "    accuracy                           0.98      3576\n",
      "   macro avg       0.88      0.94      0.91      3576\n",
      "weighted avg       0.98      0.98      0.98      3576\n",
      "\n",
      "F1 Score (Fake Jobs, class 1): 0.8223\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted Real (0)  Predicted Fake (1)\n",
      "Actual Real (0)                3354                  49\n",
      "Actual Fake (1)                  18                 155\n",
      "----------------------------------------\n",
      "Training XGBoost Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daree\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [11:44:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "## üöÄ XGBoost Model Evaluation (with Class Weighting) ##\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3403\n",
      "           1       0.83      0.82      0.82       173\n",
      "\n",
      "    accuracy                           0.98      3576\n",
      "   macro avg       0.91      0.90      0.91      3576\n",
      "weighted avg       0.98      0.98      0.98      3576\n",
      "\n",
      "F1 Score (Fake Jobs, class 1): 0.8222\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted Real (0)  Predicted Fake (1)\n",
      "Actual Real (0)                3374                  29\n",
      "Actual Fake (1)                  32                 141\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- 1. Essential Setup and NLTK Fix ---\n",
    "\n",
    "# Ensure NLTK resources are downloaded (This fixes the previous error)\n",
    "try:\n",
    "    # Use find to check if the resource is available\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"NLTK stopwords data is already downloaded.\")\n",
    "except LookupError:\n",
    "    # If not found, download it\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    # Using the standard download function\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "\n",
    "## --- 2. Data Loading and Cleaning ---\n",
    "\n",
    "# Load the dataset\n",
    "# NOTE: Replace 'fake_job_postings.csv' with your actual file name\n",
    "try:\n",
    "    df = pd.read_csv('fake_job_postings.csv') \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Please ensure 'fake_job_postings.csv' is in the same directory as this notebook.\")\n",
    "    # Exiting the script if the file is not found\n",
    "    # In a real notebook, you would stop here or load data from another source.\n",
    "    # For this demonstration, we'll assume the file is correctly placed.\n",
    "    raise\n",
    "\n",
    "# Rename the target column for clarity (assuming it's named 'fraudulent')\n",
    "df = df.rename(columns={'fraudulent': 'is_fake'})\n",
    "\n",
    "# Drop unneeded columns\n",
    "df = df.drop(columns=['salary_range', 'job_id'], errors='ignore')\n",
    "\n",
    "# Identify the target variable\n",
    "TARGET = 'is_fake'\n",
    "X = df.drop(TARGET, axis=1)\n",
    "y = df[TARGET]\n",
    "\n",
    "print(f\"\\nOriginal Dataset Shape: {df.shape}\")\n",
    "print(f\"Class Distribution (0=Real, 1=Fake):\\n{y.value_counts()}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Function for text cleaning\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Convert to string and handle HTML tags\n",
    "    text = re.sub(r'<.*?>', '', str(text))\n",
    "    # Remove non-alphanumeric characters and lower case\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    # Remove stop words and single characters\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS and len(word) > 1)\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to all relevant text columns\n",
    "text_cols = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "for col in text_cols:\n",
    "    X[col] = df[col].apply(clean_text)\n",
    "\n",
    "# Combine all cleaned text into a single feature for the TF-IDF model\n",
    "X['combined_text'] = X[text_cols].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "\n",
    "## --- 3. Feature Engineering and Preprocessing Pipeline ---\n",
    "\n",
    "# Identify column types\n",
    "text_feature = 'combined_text'\n",
    "binary_features = ['telecommuting', 'has_company_logo', 'has_questions']\n",
    "# Note: 'location' is often too high-cardinality for OHE but we use it here as per standard approach.\n",
    "categorical_features = ['location', 'employment_type', 'required_experience', 'required_education', 'function', 'industry']\n",
    "\n",
    "# Fill missing values:\n",
    "# For categorical columns, fill NaNs with a unique string 'Missing'\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].fillna('Missing')\n",
    "    \n",
    "# For binary features, ensure they are 0/1 integers\n",
    "for col in binary_features:\n",
    "    X[col] = X[col].fillna(0).astype(int) \n",
    "\n",
    "\n",
    "# Create the Column Transformer (Preprocessor)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # 1. TF-IDF for the combined text (Max features prevents dimensionality explosion)\n",
    "        ('text_vec', \n",
    "         TfidfVectorizer(ngram_range=(1, 2), max_features=10000), \n",
    "         text_feature),\n",
    "        \n",
    "        # 2. One-Hot Encoding for categorical features\n",
    "        ('cat', \n",
    "         OneHotEncoder(handle_unknown='ignore', sparse_output=True), # Use sparse output for memory efficiency\n",
    "         categorical_features),\n",
    "         \n",
    "        # 3. Pass through binary features\n",
    "        ('bin', \n",
    "         'passthrough', \n",
    "         binary_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "## --- 4. Splitting Data and Handling Imbalance (SMOTE) ---\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train/Test Split: {X_train.shape[0]} / {X_test.shape[0]}\")\n",
    "print(f\"Train Fake Ratio: {y_train.mean():.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# SMOTE is used inside the pipeline to only apply to the training data *after* transformation\n",
    "smote_sampler = SMOTE(random_state=42)\n",
    "\n",
    "## --- 5. Model Training: Logistic Regression (Baseline) ---\n",
    "\n",
    "# Use ImbPipeline to chain Preprocessor, SMOTE, and the Model\n",
    "logreg_model = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', smote_sampler), # Applies SMOTE on the vectorized, one-hot encoded training data\n",
    "    ('classifier', LogisticRegression(solver='liblinear', random_state=42, C=1.0))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Logistic Regression Model...\")\n",
    "logreg_model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "print(\"\\n## üéØ Logistic Regression Model Evaluation (with SMOTE) ##\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(f\"F1 Score (Fake Jobs, class 1): {f1_score(y_test, y_pred_logreg, pos_label=1):.4f}\")\n",
    "\n",
    "# Display Confusion Matrix\n",
    "cm_logreg = confusion_matrix(y_test, y_pred_logreg)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(cm_logreg, index=['Actual Real (0)', 'Actual Fake (1)'], columns=['Predicted Real (0)', 'Predicted Fake (1)']))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "## --- 6. Model Training: XGBoost (Recommended Performer) ---\n",
    "\n",
    "# We don't use SMOTE here, but rely on XGBoost's built-in class weighting\n",
    "# ratio = total_negative_samples / total_positive_samples\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "xgb_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), # Use the same preprocessor\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        # Crucial for imbalance: tells XGBoost to give more importance to the fake class\n",
    "        scale_pos_weight=scale_pos_weight \n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training XGBoost Model...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"\\n## üöÄ XGBoost Model Evaluation (with Class Weighting) ##\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(f\"F1 Score (Fake Jobs, class 1): {f1_score(y_test, y_pred_xgb, pos_label=1):.4f}\")\n",
    "\n",
    "# Display Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(cm_xgb, index=['Actual Real (0)', 'Actual Fake (1)'], columns=['Predicted Real (0)', 'Predicted Fake (1)']))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Final step: Choose the model with the higher F1 Score for class 1 (Fake Jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c0bcb5-24e6-49f0-a3ca-a4ec379d9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Randomized Search for XGBoost Hyperparameters...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daree\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [13:57:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Search complete.\n",
      "\n",
      "==================================================\n",
      "## üèÜ FINAL TUNED XGBOOST MODEL EVALUATION ##\n",
      "==================================================\n",
      "Best Cross-Validation F1-Score: 0.7997\n",
      "Best Hyperparameters Found:\n",
      "  colsample_bytree: 0.6943386448447411\n",
      "  learning_rate: 0.09140470953216877\n",
      "  max_depth: 7\n",
      "  n_estimators: 379\n",
      "  subsample: 0.7427013306774357\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3403\n",
      "           1       0.92      0.84      0.88       173\n",
      "\n",
      "    accuracy                           0.99      3576\n",
      "   macro avg       0.96      0.92      0.94      3576\n",
      "weighted avg       0.99      0.99      0.99      3576\n",
      "\n",
      "F1 Score (Fake Jobs, class 1): 0.8795\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted Real (0)  Predicted Fake (1)\n",
      "Actual Real (0)                3390                  13\n",
      "Actual Fake (1)                  27                 146\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "# --- ASSUMPTION: The following variables are already defined from previous data processing steps: ---\n",
    "# preprocessor, X_train, y_train, X_test, y_test, scale_pos_weight\n",
    "# \n",
    "# Example placeholder values for this standalone block to function (These should be replaced \n",
    "# by your actual variables if running this block alone without the setup steps):\n",
    "# \n",
    "# scale_pos_weight = 19.66 # Example value based on a typical 5% fake job rate\n",
    "# preprocessor = ... \n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Define the full pipeline structure\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        # Set the imbalance weight here\n",
    "        scale_pos_weight=scale_pos_weight \n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Define the parameter search space for the classifier step\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': sp_randint(100, 500), \n",
    "    'classifier__max_depth': sp_randint(3, 10),       \n",
    "    'classifier__learning_rate': sp_uniform(0.01, 0.3), \n",
    "    'classifier__colsample_bytree': sp_uniform(0.5, 0.5), \n",
    "    'classifier__subsample': sp_uniform(0.6, 0.4)       \n",
    "}\n",
    "\n",
    "# 3. Setup Randomized Search Cross-Validation\n",
    "# Scoring is set to 'f1' (F1-score) to prioritize performance on the minority class (Fake Jobs)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50, # Number of parameter settings that are sampled (adjust based on time/resources)\n",
    "    scoring='f1', \n",
    "    cv=3, # Number of cross-validation folds\n",
    "    verbose=2, \n",
    "    random_state=42,\n",
    "    n_jobs=-1 # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# 4. Run the search\n",
    "print(\"Starting Randomized Search for XGBoost Hyperparameters...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Randomized Search complete.\")\n",
    "\n",
    "\n",
    "# 5. Final Model Evaluation\n",
    "\n",
    "# Get the best estimator found by the search\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_tuned = best_xgb_model.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"## üèÜ FINAL TUNED XGBOOST MODEL EVALUATION ##\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Cross-Validation F1-Score: {best_score:.4f}\")\n",
    "print(\"Best Hyperparameters Found:\")\n",
    "# Print only the classifier parameters for cleaner output\n",
    "for k, v in best_params.items():\n",
    "    if k.startswith('classifier__'):\n",
    "        print(f\"  {k.split('__')[1]}: {v}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "print(f\"F1 Score (Fake Jobs, class 1): {f1_score(y_test, y_pred_tuned, pos_label=1):.4f}\")\n",
    "\n",
    "# Display Confusion Matrix\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(cm_tuned, index=['Actual Real (0)', 'Actual Fake (1)'], columns=['Predicted Real (0)', 'Predicted Fake (1)']))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c1c4f5-561d-4a74-bf17-5d55566619ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully saved the final model to: xgb_fake_job_detector_f1_0.8795_1765280759.pkl\n",
      "The model can now be loaded and used for new predictions.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "# Get the best model that resulted from the RandomizedSearchCV process\n",
    "# Note: This assumes 'best_xgb_model' is defined from the previous code block (Step 6)\n",
    "final_model = best_xgb_model\n",
    "\n",
    "# Create a unique filename that includes the current F1 score and a timestamp\n",
    "f1_score_str = f1_score(y_test, y_pred_tuned, pos_label=1)\n",
    "filename = f'xgb_fake_job_detector_f1_{f1_score_str:.4f}_{int(time.time())}.pkl'\n",
    "\n",
    "# Save the model to disk\n",
    "try:\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    print(f\"\\n‚úÖ Successfully saved the final model to: {filename}\")\n",
    "    print(\"The model can now be loaded and used for new predictions.\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n‚ö†Ô∏è ERROR: The variable 'best_xgb_model' or necessary data (y_test, y_pred_tuned) was not found.\")\n",
    "    print(\"Please ensure you run the Hyperparameter Tuning block completely before attempting to save the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred while saving: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fceec262-d3bb-4d6e-81eb-d02613c03a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best model successfully retrieved from the 'random_search' object.\n",
      "\n",
      "--- Testing REAL Sample (Expected 0) ---\n",
      "Prediction: REAL/LEGITIMATE (0)\n",
      "Confidence: 0.9927\n",
      "\n",
      "--- Testing FAKE Sample (Expected 1) ---\n",
      "Prediction: FAKE/FRAUDULENT (1)\n",
      "Confidence: 0.9995\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- ASSUMPTION: The following variables/objects must be defined in the current kernel: ---\n",
    "# random_search (the result of the RandomizedSearchCV run)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 1. Define required functions (copied from the training script) ---\n",
    "\n",
    "# Function for text cleaning\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'<.*?>', '', str(text))\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS and len(word) > 1)\n",
    "    return text\n",
    "\n",
    "# --- 2. Retrieve the Best Model ---\n",
    "\n",
    "try:\n",
    "    loaded_model = random_search.best_estimator_\n",
    "    print(\"‚úÖ Best model successfully retrieved from the 'random_search' object.\\n\")\n",
    "except NameError:\n",
    "    print(\"\\n‚ùå ERROR: The 'random_search' object was not found in the kernel.\")\n",
    "    raise\n",
    "\n",
    "# --- 3. Create New Job Posting DataFrames ---\n",
    "\n",
    "# Sample 1: Looks REAL\n",
    "real_sample = {\n",
    "    'title': ['Senior Data Scientist'], 'location': ['London, UK'], 'department': ['Analytics'],\n",
    "    'company_profile': ['A multinational technology company with over 10,000 employees globally, specializing in AI software development. We offer competitive salaries and excellent benefits.'],\n",
    "    'description': ['Lead a team focused on developing next-generation predictive models. Must be proficient in Python, SQL, and cloud platforms.'],\n",
    "    'requirements': ['PhD in Computer Science or related field. 5+ years experience in a senior role.'],\n",
    "    'benefits': ['Healthcare, 401K match, remote flexibility, annual bonus.'],\n",
    "    'employment_type': ['Full-time'], 'required_experience': ['Director'], 'required_education': ['Doctorate'],\n",
    "    'industry': ['Information Technology and Services'], 'function': ['Information Technology'],\n",
    "    'telecommuting': [0], 'has_company_logo': [1], 'has_questions': [1],\n",
    "}\n",
    "\n",
    "# Sample 2: Looks FAKE (Contains NaNs in department and required_education)\n",
    "fake_sample = {\n",
    "    'title': ['WORK FROM HOME - AMAZING CASH'], 'location': ['US, Remote'], 'department': [np.nan],\n",
    "    'company_profile': ['Be your own boss! Financial independence guaranteed. No experience necessary. Apply now to get rich quick!'],\n",
    "    'description': ['Simple data entry job that requires NO upfront fee. Earn $500 per day doing simple tasks from your smartphone or tablet!'],\n",
    "    'requirements': ['Must have internet access.'], 'benefits': ['Fast cash, zero commute.'],\n",
    "    'employment_type': ['Part-time'], 'required_experience': ['Entry level'], 'required_education': [np.nan],\n",
    "    'industry': ['Accounting'], 'function': ['Administrative'],\n",
    "    'telecommuting': [1], 'has_company_logo': [0], 'has_questions': [0],\n",
    "}\n",
    "\n",
    "df_test_real = pd.DataFrame(real_sample)\n",
    "df_test_fake = pd.DataFrame(fake_sample)\n",
    "\n",
    "\n",
    "# --- 4. Process and Predict Function (FIXED) ---\n",
    "\n",
    "def predict_job_fraud(df_new):\n",
    "    # --- FIX START ---\n",
    "    # Categorical columns that were imputed with 'Missing' during training MUST be imputed here.\n",
    "    categorical_features = ['location', 'employment_type', 'required_experience', \n",
    "                            'required_education', 'function', 'industry', 'department']\n",
    "    for col in categorical_features:\n",
    "        # Fill all NaNs with the string 'Missing'\n",
    "        df_new[col] = df_new[col].fillna('Missing')\n",
    "    \n",
    "    # Binary features that were imputed with 0 during training MUST be imputed here.\n",
    "    binary_features = ['telecommuting', 'has_company_logo', 'has_questions']\n",
    "    for col in binary_features:\n",
    "        df_new[col] = df_new[col].fillna(0).astype(int) \n",
    "    # --- FIX END ---\n",
    "    \n",
    "    # Text cleaning and combining\n",
    "    text_cols = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "    for col in text_cols:\n",
    "        df_new[col] = df_new[col].apply(clean_text)\n",
    "    \n",
    "    # Create the combined_text feature which the model pipeline expects\n",
    "    df_new['combined_text'] = df_new[text_cols].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    # Use the loaded model pipeline to predict\n",
    "    prediction = loaded_model.predict(df_new)\n",
    "    probability = loaded_model.predict_proba(df_new)\n",
    "    \n",
    "    result = \"FAKE/FRAUDULENT (1)\" if prediction[0] == 1 else \"REAL/LEGITIMATE (0)\"\n",
    "    confidence = probability[0][prediction[0]]\n",
    "    \n",
    "    return result, confidence\n",
    "\n",
    "\n",
    "# --- 5. Run Tests and Output Results ---\n",
    "\n",
    "print(\"--- Testing REAL Sample (Expected 0) ---\")\n",
    "result_real, confidence_real = predict_job_fraud(df_test_real.copy())\n",
    "print(f\"Prediction: {result_real}\")\n",
    "print(f\"Confidence: {confidence_real:.4f}\\n\")\n",
    "\n",
    "print(\"--- Testing FAKE Sample (Expected 1) ---\")\n",
    "result_fake, confidence_fake = predict_job_fraud(df_test_fake.copy())\n",
    "print(f\"Prediction: {result_fake}\")\n",
    "print(f\"Confidence: {confidence_fake:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd25d5-b85b-4bf2-a554-2eb085e8b81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
